From 1c99e97919ac12e8ac84b50f5c3185bcf67a49ef Mon Sep 17 00:00:00 2001
From: revsic <revsic99@gmail.com>
Date: Thu, 3 Sep 2020 00:37:46 +0900
Subject: [PATCH] cpp extension: Use native instead of cpp

---
 op/fused_act.py | 118 ++++++++++----------
 op/upfirdn2d.py | 286 ++++++++++++++++++++++++------------------------
 2 files changed, 202 insertions(+), 202 deletions(-)

diff --git a/op/fused_act.py b/op/fused_act.py
index e734e2c..140145f 100755
--- a/op/fused_act.py
+++ b/op/fused_act.py
@@ -7,68 +7,68 @@ from torch.autograd import Function
 from torch.utils.cpp_extension import load
 
 
-module_path = os.path.dirname(__file__)
-fused = load(
-    "fused",
-    sources=[
-        os.path.join(module_path, "fused_bias_act.cpp"),
-        os.path.join(module_path, "fused_bias_act_kernel.cu"),
-    ],
-)
-
-
-class FusedLeakyReLUFunctionBackward(Function):
-    @staticmethod
-    def forward(ctx, grad_output, out, negative_slope, scale):
-        ctx.save_for_backward(out)
-        ctx.negative_slope = negative_slope
-        ctx.scale = scale
-
-        empty = grad_output.new_empty(0)
-
-        grad_input = fused.fused_bias_act(
-            grad_output, empty, out, 3, 1, negative_slope, scale
-        )
+# module_path = os.path.dirname(__file__)
+# fused = load(
+#     "fused",
+#     sources=[
+#         os.path.join(module_path, "fused_bias_act.cpp"),
+#         os.path.join(module_path, "fused_bias_act_kernel.cu"),
+#     ],
+# )
 
-        dim = [0]
 
-        if grad_input.ndim > 2:
-            dim += list(range(2, grad_input.ndim))
+# class FusedLeakyReLUFunctionBackward(Function):
+#     @staticmethod
+#     def forward(ctx, grad_output, out, negative_slope, scale):
+#         ctx.save_for_backward(out)
+#         ctx.negative_slope = negative_slope
+#         ctx.scale = scale
 
-        grad_bias = grad_input.sum(dim).detach()
+#         empty = grad_output.new_empty(0)
 
-        return grad_input, grad_bias
+#         grad_input = fused.fused_bias_act(
+#             grad_output, empty, out, 3, 1, negative_slope, scale
+#         )
 
-    @staticmethod
-    def backward(ctx, gradgrad_input, gradgrad_bias):
-        out, = ctx.saved_tensors
-        gradgrad_out = fused.fused_bias_act(
-            gradgrad_input, gradgrad_bias, out, 3, 1, ctx.negative_slope, ctx.scale
-        )
+#         dim = [0]
 
-        return gradgrad_out, None, None, None
+#         if grad_input.ndim > 2:
+#             dim += list(range(2, grad_input.ndim))
 
+#         grad_bias = grad_input.sum(dim).detach()
 
-class FusedLeakyReLUFunction(Function):
-    @staticmethod
-    def forward(ctx, input, bias, negative_slope, scale):
-        empty = input.new_empty(0)
-        out = fused.fused_bias_act(input, bias, empty, 3, 0, negative_slope, scale)
-        ctx.save_for_backward(out)
-        ctx.negative_slope = negative_slope
-        ctx.scale = scale
+#         return grad_input, grad_bias
 
-        return out
+#     @staticmethod
+#     def backward(ctx, gradgrad_input, gradgrad_bias):
+#         out, = ctx.saved_tensors
+#         gradgrad_out = fused.fused_bias_act(
+#             gradgrad_input, gradgrad_bias, out, 3, 1, ctx.negative_slope, ctx.scale
+#         )
 
-    @staticmethod
-    def backward(ctx, grad_output):
-        out, = ctx.saved_tensors
+#         return gradgrad_out, None, None, None
 
-        grad_input, grad_bias = FusedLeakyReLUFunctionBackward.apply(
-            grad_output, out, ctx.negative_slope, ctx.scale
-        )
 
-        return grad_input, grad_bias, None, None
+# class FusedLeakyReLUFunction(Function):
+#     @staticmethod
+#     def forward(ctx, input, bias, negative_slope, scale):
+#         empty = input.new_empty(0)
+#         out = fused.fused_bias_act(input, bias, empty, 3, 0, negative_slope, scale)
+#         ctx.save_for_backward(out)
+#         ctx.negative_slope = negative_slope
+#         ctx.scale = scale
+
+#         return out
+
+#     @staticmethod
+#     def backward(ctx, grad_output):
+#         out, = ctx.saved_tensors
+
+#         grad_input, grad_bias = FusedLeakyReLUFunctionBackward.apply(
+#             grad_output, out, ctx.negative_slope, ctx.scale
+#         )
+
+#         return grad_input, grad_bias, None, None
 
 
 class FusedLeakyReLU(nn.Module):
@@ -84,14 +84,14 @@ class FusedLeakyReLU(nn.Module):
 
 
 def fused_leaky_relu(input, bias, negative_slope=0.2, scale=2 ** 0.5):
-    if input.device.type == "cpu":
-        rest_dim = [1] * (input.ndim - bias.ndim - 1)
-        return (
-            F.leaky_relu(
-                input + bias.view(1, bias.shape[0], *rest_dim), negative_slope=0.2
-            )
-            * scale
+    # if input.device.type == "cpu":
+    rest_dim = [1] * (input.ndim - bias.ndim - 1)
+    return (
+        F.leaky_relu(
+            input + bias.view(1, bias.shape[0], *rest_dim), negative_slope=0.2
         )
+        * scale
+    )
 
-    else:
-        return FusedLeakyReLUFunction.apply(input, bias, negative_slope, scale)
+    # else:
+    #     return FusedLeakyReLUFunction.apply(input, bias, negative_slope, scale)
diff --git a/op/upfirdn2d.py b/op/upfirdn2d.py
index a4cf05d..5d6f6c8 100755
--- a/op/upfirdn2d.py
+++ b/op/upfirdn2d.py
@@ -6,152 +6,152 @@ from torch.autograd import Function
 from torch.utils.cpp_extension import load
 
 
-module_path = os.path.dirname(__file__)
-upfirdn2d_op = load(
-    "upfirdn2d",
-    sources=[
-        os.path.join(module_path, "upfirdn2d.cpp"),
-        os.path.join(module_path, "upfirdn2d_kernel.cu"),
-    ],
-)
-
-
-class UpFirDn2dBackward(Function):
-    @staticmethod
-    def forward(
-        ctx, grad_output, kernel, grad_kernel, up, down, pad, g_pad, in_size, out_size
-    ):
-
-        up_x, up_y = up
-        down_x, down_y = down
-        g_pad_x0, g_pad_x1, g_pad_y0, g_pad_y1 = g_pad
-
-        grad_output = grad_output.reshape(-1, out_size[0], out_size[1], 1)
-
-        grad_input = upfirdn2d_op.upfirdn2d(
-            grad_output,
-            grad_kernel,
-            down_x,
-            down_y,
-            up_x,
-            up_y,
-            g_pad_x0,
-            g_pad_x1,
-            g_pad_y0,
-            g_pad_y1,
-        )
-        grad_input = grad_input.view(in_size[0], in_size[1], in_size[2], in_size[3])
-
-        ctx.save_for_backward(kernel)
-
-        pad_x0, pad_x1, pad_y0, pad_y1 = pad
-
-        ctx.up_x = up_x
-        ctx.up_y = up_y
-        ctx.down_x = down_x
-        ctx.down_y = down_y
-        ctx.pad_x0 = pad_x0
-        ctx.pad_x1 = pad_x1
-        ctx.pad_y0 = pad_y0
-        ctx.pad_y1 = pad_y1
-        ctx.in_size = in_size
-        ctx.out_size = out_size
-
-        return grad_input
-
-    @staticmethod
-    def backward(ctx, gradgrad_input):
-        kernel, = ctx.saved_tensors
-
-        gradgrad_input = gradgrad_input.reshape(-1, ctx.in_size[2], ctx.in_size[3], 1)
-
-        gradgrad_out = upfirdn2d_op.upfirdn2d(
-            gradgrad_input,
-            kernel,
-            ctx.up_x,
-            ctx.up_y,
-            ctx.down_x,
-            ctx.down_y,
-            ctx.pad_x0,
-            ctx.pad_x1,
-            ctx.pad_y0,
-            ctx.pad_y1,
-        )
-        # gradgrad_out = gradgrad_out.view(ctx.in_size[0], ctx.out_size[0], ctx.out_size[1], ctx.in_size[3])
-        gradgrad_out = gradgrad_out.view(
-            ctx.in_size[0], ctx.in_size[1], ctx.out_size[0], ctx.out_size[1]
-        )
-
-        return gradgrad_out, None, None, None, None, None, None, None, None
-
-
-class UpFirDn2d(Function):
-    @staticmethod
-    def forward(ctx, input, kernel, up, down, pad):
-        up_x, up_y = up
-        down_x, down_y = down
-        pad_x0, pad_x1, pad_y0, pad_y1 = pad
-
-        kernel_h, kernel_w = kernel.shape
-        batch, channel, in_h, in_w = input.shape
-        ctx.in_size = input.shape
-
-        input = input.reshape(-1, in_h, in_w, 1)
-
-        ctx.save_for_backward(kernel, torch.flip(kernel, [0, 1]))
-
-        out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h) // down_y + 1
-        out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w) // down_x + 1
-        ctx.out_size = (out_h, out_w)
-
-        ctx.up = (up_x, up_y)
-        ctx.down = (down_x, down_y)
-        ctx.pad = (pad_x0, pad_x1, pad_y0, pad_y1)
-
-        g_pad_x0 = kernel_w - pad_x0 - 1
-        g_pad_y0 = kernel_h - pad_y0 - 1
-        g_pad_x1 = in_w * up_x - out_w * down_x + pad_x0 - up_x + 1
-        g_pad_y1 = in_h * up_y - out_h * down_y + pad_y0 - up_y + 1
-
-        ctx.g_pad = (g_pad_x0, g_pad_x1, g_pad_y0, g_pad_y1)
-
-        out = upfirdn2d_op.upfirdn2d(
-            input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1
-        )
-        # out = out.view(major, out_h, out_w, minor)
-        out = out.view(-1, channel, out_h, out_w)
-
-        return out
-
-    @staticmethod
-    def backward(ctx, grad_output):
-        kernel, grad_kernel = ctx.saved_tensors
-
-        grad_input = UpFirDn2dBackward.apply(
-            grad_output,
-            kernel,
-            grad_kernel,
-            ctx.up,
-            ctx.down,
-            ctx.pad,
-            ctx.g_pad,
-            ctx.in_size,
-            ctx.out_size,
-        )
-
-        return grad_input, None, None, None, None
+# module_path = os.path.dirname(__file__)
+# upfirdn2d_op = load(
+#     "upfirdn2d",
+#     sources=[
+#         os.path.join(module_path, "upfirdn2d.cpp"),
+#         os.path.join(module_path, "upfirdn2d_kernel.cu"),
+#     ],
+# )
+
+
+# class UpFirDn2dBackward(Function):
+#     @staticmethod
+#     def forward(
+#         ctx, grad_output, kernel, grad_kernel, up, down, pad, g_pad, in_size, out_size
+#     ):
+
+#         up_x, up_y = up
+#         down_x, down_y = down
+#         g_pad_x0, g_pad_x1, g_pad_y0, g_pad_y1 = g_pad
+
+#         grad_output = grad_output.reshape(-1, out_size[0], out_size[1], 1)
+
+#         grad_input = upfirdn2d_op.upfirdn2d(
+#             grad_output,
+#             grad_kernel,
+#             down_x,
+#             down_y,
+#             up_x,
+#             up_y,
+#             g_pad_x0,
+#             g_pad_x1,
+#             g_pad_y0,
+#             g_pad_y1,
+#         )
+#         grad_input = grad_input.view(in_size[0], in_size[1], in_size[2], in_size[3])
+
+#         ctx.save_for_backward(kernel)
+
+#         pad_x0, pad_x1, pad_y0, pad_y1 = pad
+
+#         ctx.up_x = up_x
+#         ctx.up_y = up_y
+#         ctx.down_x = down_x
+#         ctx.down_y = down_y
+#         ctx.pad_x0 = pad_x0
+#         ctx.pad_x1 = pad_x1
+#         ctx.pad_y0 = pad_y0
+#         ctx.pad_y1 = pad_y1
+#         ctx.in_size = in_size
+#         ctx.out_size = out_size
+
+#         return grad_input
+
+#     @staticmethod
+#     def backward(ctx, gradgrad_input):
+#         kernel, = ctx.saved_tensors
+
+#         gradgrad_input = gradgrad_input.reshape(-1, ctx.in_size[2], ctx.in_size[3], 1)
+
+#         gradgrad_out = upfirdn2d_op.upfirdn2d(
+#             gradgrad_input,
+#             kernel,
+#             ctx.up_x,
+#             ctx.up_y,
+#             ctx.down_x,
+#             ctx.down_y,
+#             ctx.pad_x0,
+#             ctx.pad_x1,
+#             ctx.pad_y0,
+#             ctx.pad_y1,
+#         )
+#         # gradgrad_out = gradgrad_out.view(ctx.in_size[0], ctx.out_size[0], ctx.out_size[1], ctx.in_size[3])
+#         gradgrad_out = gradgrad_out.view(
+#             ctx.in_size[0], ctx.in_size[1], ctx.out_size[0], ctx.out_size[1]
+#         )
+
+#         return gradgrad_out, None, None, None, None, None, None, None, None
+
+
+# class UpFirDn2d(Function):
+#     @staticmethod
+#     def forward(ctx, input, kernel, up, down, pad):
+#         up_x, up_y = up
+#         down_x, down_y = down
+#         pad_x0, pad_x1, pad_y0, pad_y1 = pad
+
+#         kernel_h, kernel_w = kernel.shape
+#         batch, channel, in_h, in_w = input.shape
+#         ctx.in_size = input.shape
+
+#         input = input.reshape(-1, in_h, in_w, 1)
+
+#         ctx.save_for_backward(kernel, torch.flip(kernel, [0, 1]))
+
+#         out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h) // down_y + 1
+#         out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w) // down_x + 1
+#         ctx.out_size = (out_h, out_w)
+
+#         ctx.up = (up_x, up_y)
+#         ctx.down = (down_x, down_y)
+#         ctx.pad = (pad_x0, pad_x1, pad_y0, pad_y1)
+
+#         g_pad_x0 = kernel_w - pad_x0 - 1
+#         g_pad_y0 = kernel_h - pad_y0 - 1
+#         g_pad_x1 = in_w * up_x - out_w * down_x + pad_x0 - up_x + 1
+#         g_pad_y1 = in_h * up_y - out_h * down_y + pad_y0 - up_y + 1
+
+#         ctx.g_pad = (g_pad_x0, g_pad_x1, g_pad_y0, g_pad_y1)
+
+#         out = upfirdn2d_op.upfirdn2d(
+#             input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1
+#         )
+#         # out = out.view(major, out_h, out_w, minor)
+#         out = out.view(-1, channel, out_h, out_w)
+
+#         return out
+
+#     @staticmethod
+#     def backward(ctx, grad_output):
+#         kernel, grad_kernel = ctx.saved_tensors
+
+#         grad_input = UpFirDn2dBackward.apply(
+#             grad_output,
+#             kernel,
+#             grad_kernel,
+#             ctx.up,
+#             ctx.down,
+#             ctx.pad,
+#             ctx.g_pad,
+#             ctx.in_size,
+#             ctx.out_size,
+#         )
+
+#         return grad_input, None, None, None, None
 
 
 def upfirdn2d(input, kernel, up=1, down=1, pad=(0, 0)):
-    if input.device.type == "cpu":
-        out = upfirdn2d_native(
-            input, kernel, up, up, down, down, pad[0], pad[1], pad[0], pad[1]
-        )
-
-    else:
-        out = UpFirDn2d.apply(
-            input, kernel, (up, up), (down, down), (pad[0], pad[1], pad[0], pad[1])
-        )
+    # if input.device.type == "cpu":
+    out = upfirdn2d_native(
+        input, kernel, up, up, down, down, pad[0], pad[1], pad[0], pad[1]
+    )
+
+    # else:
+    #     out = UpFirDn2d.apply(
+    #         input, kernel, (up, up), (down, down), (pad[0], pad[1], pad[0], pad[1])
+    #     )
 
     return out
 
-- 
2.21.0.windows.1

